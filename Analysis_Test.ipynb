{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This python script purpose is to clean spatial data files that will be used in \n",
    "future analysis. Cleaning the data includes removing fields that are not needed, \n",
    "removing invalid geometries from the geodataframes, adding geometery where there \n",
    "is none, and clipping all the data to the 7 county metro as the area of interest.\n",
    "'''\n",
    "\n",
    "# Loading all shapefile datasets in as geopandas dataframes\n",
    "hydrography = gpd.read_file(\"zip://shp_water_dnr_hydrography.zip\")\n",
    "water2018 = gpd.read_file(\"zip://impaired_2018_lakes.zip\")\n",
    "water2016 = gpd.read_file(\"zip://impaired_2016_lakes.zip\")\n",
    "water2014 = gpd.read_file(\"zip://impaired_2014_lakes.zip\")\n",
    "metro = gpd.read_file(\"zip://shp_bdry_metro_counties_and_ctus.zip\")\n",
    "\n",
    "###\n",
    "### CLEANING AND CLIPPING IMPAIRED WATER 2014, 2016, AND 2018\n",
    "###\n",
    "\n",
    "# Cleaning the metro dataset, dissolving on the county name. \n",
    "metro_dissolve = metro.dissolve(by = \"CO_NAME\")\n",
    "\n",
    "# Dropping all the unnecessary columns\n",
    "water2018 = water2018.drop([\"CAT\", \"CAT_DESC\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"LIKE_MEET\", \n",
    "                            \"NON_POLL\", \"NAT_BACK\", \"ADD_MON\", \"APPROVED\", \"NEEDS_PLN\", \"IMP_PARAM\", \"NEW_IMPAIR\", \n",
    "                            \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \"AMMONIA\", \"CHLORIDE\", \n",
    "                            \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"Shape_Leng\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "water2016 = water2016.drop([\"CAT\", \"DATASET_NA\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"TMDL_NOT_R\", \n",
    "                            \"TMDL_NOT_1\", \"IMPAIR_PAR\", \"IMPAIR_P_1\", \"NEW_IMPAIR\", \"NEW_IMPA_1\", \"TMDL_APPRO\", \"TMDL_APP_1\", \n",
    "                            \"TMDL_NEEDE\", \"TMDL_NEE_1\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \n",
    "                            \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"SHAPE_Leng\", \"SHAPE_Area\"], axis = 1)\n",
    "\n",
    "water2014 = water2014.drop([\"LOCATION\", \"CAT\", \"AFFECTED_U\", \"NOPLN\", \"APPROVED\", \"NEEDSPLN\", \"IMPAIR_PAR\", \n",
    "                            \"NEW_2014\", \"HUC8\", \"HUC8_NAME\", \"HUC4\", \"BASIN\", \"WDWMO_NAME\", \"WDWMO_TYPE\", \"Chloride\", \n",
    "                            \"HgF\", \"HgW\", \"Nutrients\", \"PCBF\", \"PFOS_W\", \"SHAPE_Leng\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2014 = water2014.rename(columns = {\"WATER_NAME\" : \"NAME\", \"ALL_COUNTI\" : \"COUNTY\", \"ACRES\" : \"AREA_ACRES\"})\n",
    "\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "water2018_drop_invalid = water2018.loc[water2018['geometry'].is_valid, :]\n",
    "\n",
    "water2016_drop_invalid = water2016.loc[water2016['geometry'].is_valid, :]\n",
    "\n",
    "water2014_drop_invalid = water2014.loc[water2014['geometry'].is_valid, :]\n",
    "\n",
    "\n",
    "# Clipping the three impaired water files to the 7 county metro\n",
    "water2018_clip = gpd.clip(water2018_drop_invalid, metro_dissolve)\n",
    "water2016_clip = gpd.clip(water2016_drop_invalid, metro_dissolve)\n",
    "\n",
    "# 2014 needed to be reprojected - then clip was performed\n",
    "water2014_proj = water2014_drop_invalid.to_crs('EPSG:26915')\n",
    "water2014_clip = gpd.clip(water2014_proj, metro_dissolve)\n",
    "\n",
    "###\n",
    "### CLEANING THE 2020 IMPAIRED WATER DATA SET\n",
    "###\n",
    "\n",
    "# Load water 2020 data csv, selecting out the columns that we want and addinga geometry column\n",
    "# and pulling out only the lake features.\n",
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n",
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"County\", \"Water body type\", \"geometry\"]]\n",
    "water2020_lake = water2020.loc[(water2020[\"Water body type\"] == \"Lake\")]\n",
    "\n",
    "# Dropping the \"water body type\" field since it is no longer needed\n",
    "water2020_lake = water2020_lake[[\"AUID\", \"Water body name\", \"County\", \"geometry\"]]\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2020_lake = water2020_lake.rename(columns = {\"Water body name\" : \"NAME\", \"County\" : \"COUNTY\"})\n",
    "\n",
    "# Selecting out the 7 county metro\n",
    "counties = [\"Anoka\", \"Hennepin\", \"Ramsey\", \"Washington\", \"Carver\", \"Scott\", \"Dakota\"]\n",
    "water2020_metro = water2020_lake.loc[(water2020_lake[\"COUNTY\"].isin(counties))]\n",
    "\n",
    "# Varifying all the correct counties are there\n",
    "water2020_metro[\"COUNTY\"].unique()\n",
    "\n",
    "# Drop Duplicate AUIDs\n",
    "water2020_clean = water2020_metro.drop_duplicates(subset = [\"AUID\"])\n",
    "\n",
    "\n",
    "# Creating a list of the gpdf to loop through and find the smallest lake size\n",
    "dfs = [water2014_clip, water2016_clip, water2018_clip]\n",
    "dfs_names = [\"water2014_clip\", \"water2016_clip\", \"water2018_clip\"]\n",
    "\n",
    "# New field for impairment status in all data sets\n",
    "for df in dfs:\n",
    "    df[\"status\"] = \"Impaired\"\n",
    "\n",
    "def find_min(dfs):\n",
    "    '''\n",
    "    finds smallest lake within the impaired datasets\n",
    "    Parameter: list of dataframes\n",
    "    '''\n",
    "    \n",
    "    global minimum\n",
    "    minimum = []\n",
    "    for df in dfs:\n",
    "        minimum.append(df[\"AREA_ACRES\"].min())\n",
    "    minimum = min(minimum)\n",
    "\n",
    "# Find smallest lake size of from all impaired lakes\n",
    "find_min(dfs)\n",
    "\n",
    "\n",
    "###\n",
    "### CLEANING THE HYDROGRAPHY DATA SET\n",
    "###\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "hydro_drop_invalid = hydrography.loc[hydrography['geometry'].is_valid, :]\n",
    "\n",
    "# Clipping hydro to the 7 county metro\n",
    "hydro_clip = gpd.clip(hydro_drop_invalid, metro_dissolve)\n",
    "\n",
    "# Narrowing down the number of features in the hydro layer to only lakes and ponds\n",
    "hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]\n",
    "\n",
    "# Selecting only the lakes that are at least the size of the the impaired water dataframes\n",
    "hydro_lake = hydro_lake.loc[(hydro_lake[\"acres\"] >= minimum)]\n",
    "\n",
    "# Dropping all excess fields from the dataframe\n",
    "hydro_clean = hydro_lake.drop([\"fw_id\", \"dowlknum\", \"sub_flag\", \"wb_class\", \"lake_class\", \"shore_mi\", \"center_utm\", \"center_u_1\",\n",
    "                               \"dnr_region\", \"fsh_office\", \"outside_mn\", \"delineated\", \"delineatio\", \"delineat_1\", \"delineat_2\", \n",
    "                               \"approved_b\", \"approval_d\", \"approval_n\", \"has_flag\", \"flag_type\", \"publish_da\", \"lksdb_basi\", \"has_wld_fl\",\n",
    "                               \"wld_flag_t\", \"created_us\", \"created_da\", \"last_edite\", \"last_edi_1\", \"ow_use\", \"pwi_class\", \"map_displa\", \n",
    "                               \"shape_Leng\", \"shape_Area\", \"INSIDE_X\", \"INSIDE_Y\", \"in_lakefin\"], axis = 1)\n",
    "\n",
    "# New field for impairment status to be used when data is joined with the impaired data sets\n",
    "hydro_clean[\"status\"] = \"\"\n",
    "\n",
    "# Dissolve hydrography geometry by lake name \n",
    "hydro_clean = hydro_clean.rename(columns={'pw_basin_n': 'NAME'})\n",
    "hydro_dis = (hydro_clean.dissolve(by='NAME')).reset_index()\n",
    "\n",
    "\n",
    "def complete_hydro(waterdata, waterdata_name):\n",
    "    '''\n",
    "    [add doc]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Combine nonimpaired with impaired. Returns a pandas dataframe\n",
    "    join_hydro = hydro_dis.merge(waterdata, on ='NAME', how='left') \n",
    "    \n",
    "    # Set geometry to hydro_dis dataset for all features\n",
    "    projected = join_hydro.set_geometry(join_hydro['geometry_x'], \n",
    "                                        crs='EPSG:26915')\n",
    "    \n",
    "    # Combine duplicate features\n",
    "    projected_dis = (projected.dissolve(by='NAME')).reset_index() \n",
    "    \n",
    "    # Remove unneccessary fields\n",
    "    projected_dis = projected_dis[['NAME', \n",
    "                                   'geometry', \n",
    "                                   'acres', \n",
    "                                   'cty_name', \n",
    "                                   'unique_id', \n",
    "                                   'status_y']]\n",
    "    \n",
    "    # Fill status of nonimpaired lakes\n",
    "    projected_dis = projected_dis.fillna(\"nonimpaired\")\n",
    "    projected_df = projected_dis.rename(columns = {'status_y': 'status'})\n",
    "\n",
    "    projected_df.to_file(f'{waterdata_name}.shp')\n",
    "\n",
    "# Nonimpaired and impaired completed dataset for each year    \n",
    "for df in dfs:\n",
    "    for name in dfs_names:\n",
    "        complete_hydro(df, name)\n",
    "\n",
    "    \n",
    "###\n",
    "### JOINING GEOMETRY TO WATER2020_CLEAN\n",
    "### \n",
    "\n",
    "water2020_join_auid = water2020_clean.merge(water2018_clip, how = \"left\", on = \"AUID\")\n",
    "\n",
    "water2020_join_auid = water2020_join_auid[[\"AUID\", \"NAME_x\", \"COUNTY_x\", \"AREA_ACRES\", \"geometry_y\", \"status\"]]\n",
    "\n",
    "water2020_join_auid = water2020_join_auid.rename(columns = {\"NAME_x\" : \"NAME\", \"COUNTY_x\" : \"COUNTY\", \"geometry_y\" : \"geometry\"})\n",
    "\n",
    "complete_hydro(water2020_join_auid, \"water2020_clip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "\n",
    "water2014_buffer = gpd.GeoDataFrame(water2014.buffer(500))\n",
    "water2014_buffer[\"NAME\"] = water2014[\"NAME\"]\n",
    "water2014_buffer = water2014_buffer.set_geometry(water2014_buffer[0])\n",
    "water2014_buffer = water2014_buffer[[\"NAME\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(input(\"Provide a distance for the size of the buffer in meters: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "water2016 = gpd.read_file(\"water2016_clip.shp\")\n",
    "water2018 = gpd.read_file(\"water2018_clip.shp\")\n",
    "water2020 = gpd.read_file(\"water2020_clip.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_lakes(buffer, water_feat):\n",
    "    lake_buffer = gpd.GeoDataFrame(water_feat.buffer(buffer))\n",
    "    lake_buffer[\"NAME\"] = water_feat[\"NAME\"]\n",
    "    lake_buffer = lake_buffer.set_geometry(lake_buffer[0])\n",
    "    lake_buffer = lake_buffer[[\"NAME\", \"geometry\"]]\n",
    "    return lake_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer2014 = buffer_lakes(buffer_size, water2014)\n",
    "buffer2016 = buffer_lakes(buffer_size, water2016)\n",
    "buffer2018 = buffer_lakes(buffer_size, water2018)\n",
    "buffer2020 = buffer_lakes(buffer_size, water2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer2014[\"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metro data by finding all files ending in _metro.zip\n",
    "directory = r'/home/leex6165/gisproj/'\n",
    "path = f'{directory}*19_metro.zip'\n",
    "\n",
    "buffer = buffer2018\n",
    "data_2018 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2018['status']})\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2018 = data_2018.merge(data_grp, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{directory}*20_metro.zip'\n",
    "\n",
    "buffer = buffer2020\n",
    "data_2020 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2020['status']} )\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2020 = data_2020.merge(data_grp, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKE 2 CELLS BELOW INTO FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of visitation for nonimpaired and impaired lakes 2020\n",
    "def vis_stats(counts_df):\n",
    "    vis = counts_df.groupby(['STATUS']).sum()\n",
    "    lkcounts = counts_df.groupby(['STATUS'])['NAME'].count()\n",
    "\n",
    "    vis['avg monthly vis'] = vis.mean(axis=1)\n",
    "    vis['Lake Counts'] = lkcounts\n",
    "    vis['Total visits'] = (vis.sum(axis=1))\n",
    "    vis['Total visits'] = vis['Total visits'] - (vis['avg monthly vis'] + vis['Lake Counts'])\n",
    "    vis['Avg monthly vis per lake'] = vis['avg monthly vis'] / vis['Lake Counts']\n",
    "    vis['Avg yearly vis per lake'] = vis['Total visits'] / vis['Lake Counts']\n",
    "    vis.loc['Total vis per month']= vis.sum(axis=0)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_stats(data_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_stats(data_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(counts_df, year):\n",
    "    total = counts_df\n",
    "    total['Total visits'] = total.sum(axis=1)\n",
    "\n",
    "    maximum = total.sort_values(by=['Total visits'], ascending=False, ignore_index=True)\n",
    "    print(f'The top five most visted lakes for {year} are: ')\n",
    "    for row in range(len(maximum[0:5])):\n",
    "        print(f\"Lake Name: {maximum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {maximum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {maximum['STATUS'][row]}\\n\")\n",
    "\n",
    "    minimum = total.sort_values(by=['Total visits'], ascending=True, ignore_index=True)\n",
    "    print(f'The top five least visted lakes for {year} are: ')\n",
    "    for row in range(len(minimum[0:5])):\n",
    "        print(f\"Lake Name: {minimum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {minimum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {minimum['STATUS'][row]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(data_2020, \"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(data_2018, \"2018\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
