{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "\n",
    "water2014_buffer = gpd.GeoDataFrame(water2014.buffer(500))\n",
    "water2014_buffer[\"NAME\"] = water2014[\"NAME\"]\n",
    "water2014_buffer = water2014_buffer.set_geometry(water2014_buffer[0])\n",
    "water2014_buffer = water2014_buffer[[\"NAME\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_buffer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(input(\"Provide a distance for the size of the buffer in meters: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "water2016 = gpd.read_file(\"water2016_clip.shp\")\n",
    "water2018 = gpd.read_file(\"water2018_clip.shp\")\n",
    "water2020 = gpd.read_file(\"water2020_clip.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_lakes(buffer, water_feat):\n",
    "    lake_buffer = gpd.GeoDataFrame(water_feat.buffer(buffer))\n",
    "    lake_buffer[\"NAME\"] = water_feat[\"NAME\"]\n",
    "    lake_buffer = lake_buffer.set_geometry(lake_buffer[0])\n",
    "    lake_buffer = lake_buffer[[\"NAME\", \"geometry\"]]\n",
    "    return lake_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer2014 = buffer_lakes(buffer_size, water2014)\n",
    "buffer2016 = buffer_lakes(buffer_size, water2016)\n",
    "buffer2018 = buffer_lakes(buffer_size, water2018)\n",
    "buffer2020 = buffer_lakes(buffer_size, water2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer2014[\"NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = water2018.loc[(water2018['status'] == 'Impaired')]\n",
    "len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metro data by finding all files ending in _metro.zip\n",
    "directory = r'/home/leex6165/gisproj/'\n",
    "path = f'{directory}*19_metro.zip'\n",
    "\n",
    "buffer = buffer2018\n",
    "data_2018 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2018['status']})\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2018 = data_2018.merge(data_grp, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{directory}*20_metro.zip'\n",
    "\n",
    "buffer = buffer2020\n",
    "data_2020 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2020['status']} )\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2020 = data_2020.merge(data_grp, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_results = test.loc[(test['NAME'] == 'Adams Hill Pond')]\n",
    "#len(check_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of points in each lake buffer.\n",
    "data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "data_grp = data_grp.rename(columns = {'index_right': 'counts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Testing how to join counts for each month and how to create one dataset later. \n",
    "Thought: create an empy dataframe with lakes as a row and months as header column names.\n",
    "Append each count to matching lake name for each monthly dataset. \n",
    "\n",
    "Other thought- since dataframes can be created with lists. Could create multiple lists\n",
    "and turn into a dataframe. \n",
    "\n",
    "i.e. data = [[lake1, 14, 10], [lake2, 10, 1]]\n",
    "pd.DataFrame(data, columns=['lake', 'apr19', 'may19'])\n",
    "'''\n",
    "test_dict = test_group.to_dict(orient='records')\n",
    "yearly_dict = [test_dict]\n",
    "yearly_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One dataframe per impaired - first column = lakes\n",
    "# Brainstorm: outer merges on df, rename counts with month. \n",
    "# add the impairment status back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
