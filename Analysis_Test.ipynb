{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date = datetime.date.today().strftime(\"%m%d%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This python script purpose is to clean spatial data files that will be used in \n",
    "future analysis. Cleaning the data includes removing fields that are not needed, \n",
    "removing invalid geometries from the geodataframes, adding geometery where there \n",
    "is none, and clipping all the data to the 7 county metro as the area of interest.\n",
    "'''\n",
    "\n",
    "# Loading all shapefile datasets in as geopandas dataframes\n",
    "hydrography = gpd.read_file(\"zip://shp_water_dnr_hydrography.zip\")\n",
    "water2018 = gpd.read_file(\"zip://impaired_2018_lakes.zip\")\n",
    "water2016 = gpd.read_file(\"zip://impaired_2016_lakes.zip\")\n",
    "water2014 = gpd.read_file(\"zip://impaired_2014_lakes.zip\")\n",
    "metro = gpd.read_file(\"zip://shp_bdry_metro_counties_and_ctus.zip\")\n",
    "\n",
    "###\n",
    "### CLEANING AND CLIPPING IMPAIRED WATER 2014, 2016, AND 2018\n",
    "###\n",
    "\n",
    "# Cleaning the metro dataset, dissolving on the county name. \n",
    "metro_dissolve = metro.dissolve(by = \"CO_NAME\")\n",
    "\n",
    "# Dropping all the unnecessary columns\n",
    "water2018 = water2018.drop([\"CAT\", \"CAT_DESC\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"LIKE_MEET\", \n",
    "                            \"NON_POLL\", \"NAT_BACK\", \"ADD_MON\", \"APPROVED\", \"NEEDS_PLN\", \"IMP_PARAM\", \"NEW_IMPAIR\", \n",
    "                            \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \"AMMONIA\", \"CHLORIDE\", \n",
    "                            \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"Shape_Leng\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "water2016 = water2016.drop([\"CAT\", \"DATASET_NA\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"TMDL_NOT_R\", \n",
    "                            \"TMDL_NOT_1\", \"IMPAIR_PAR\", \"IMPAIR_P_1\", \"NEW_IMPAIR\", \"NEW_IMPA_1\", \"TMDL_APPRO\", \"TMDL_APP_1\", \n",
    "                            \"TMDL_NEEDE\", \"TMDL_NEE_1\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \n",
    "                            \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"SHAPE_Leng\", \"SHAPE_Area\"], axis = 1)\n",
    "\n",
    "water2014 = water2014.drop([\"LOCATION\", \"CAT\", \"AFFECTED_U\", \"NOPLN\", \"APPROVED\", \"NEEDSPLN\", \"IMPAIR_PAR\", \n",
    "                            \"NEW_2014\", \"HUC8\", \"HUC8_NAME\", \"HUC4\", \"BASIN\", \"WDWMO_NAME\", \"WDWMO_TYPE\", \"Chloride\", \n",
    "                            \"HgF\", \"HgW\", \"Nutrients\", \"PCBF\", \"PFOS_W\", \"SHAPE_Leng\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2014 = water2014.rename(columns = {\"WATER_NAME\" : \"NAME\", \"ALL_COUNTI\" : \"COUNTY\", \"ACRES\" : \"AREA_ACRES\"})\n",
    "\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "water2018_drop_invalid = water2018.loc[water2018['geometry'].is_valid, :]\n",
    "\n",
    "water2016_drop_invalid = water2016.loc[water2016['geometry'].is_valid, :]\n",
    "\n",
    "water2014_drop_invalid = water2014.loc[water2014['geometry'].is_valid, :]\n",
    "\n",
    "\n",
    "# Clipping the three impaired water files to the 7 county metro\n",
    "water2018_clip = gpd.clip(water2018_drop_invalid, metro_dissolve)\n",
    "water2016_clip = gpd.clip(water2016_drop_invalid, metro_dissolve)\n",
    "\n",
    "# 2014 needed to be reprojected - then clip was performed\n",
    "water2014_proj = water2014_drop_invalid.to_crs('EPSG:26915')\n",
    "water2014_clip = gpd.clip(water2014_proj, metro_dissolve)\n",
    "\n",
    "###\n",
    "### CLEANING THE 2020 IMPAIRED WATER DATA SET\n",
    "###\n",
    "\n",
    "# Load water 2020 data csv, selecting out the columns that we want and addinga geometry column\n",
    "# and pulling out only the lake features.\n",
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n",
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"County\", \"Water body type\", \"geometry\"]]\n",
    "water2020_lake = water2020.loc[(water2020[\"Water body type\"] == \"Lake\")]\n",
    "\n",
    "# Dropping the \"water body type\" field since it is no longer needed\n",
    "water2020_lake = water2020_lake[[\"AUID\", \"Water body name\", \"County\", \"geometry\"]]\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2020_lake = water2020_lake.rename(columns = {\"Water body name\" : \"NAME\", \"County\" : \"COUNTY\"})\n",
    "\n",
    "# Selecting out the 7 county metro\n",
    "counties = [\"Anoka\", \"Hennepin\", \"Ramsey\", \"Washington\", \"Carver\", \"Scott\", \"Dakota\"]\n",
    "water2020_metro = water2020_lake.loc[(water2020_lake[\"COUNTY\"].isin(counties))]\n",
    "\n",
    "# Varifying all the correct counties are there\n",
    "water2020_metro[\"COUNTY\"].unique()\n",
    "\n",
    "# Drop Duplicate AUIDs\n",
    "water2020_clean = water2020_metro.drop_duplicates(subset = [\"AUID\"])\n",
    "\n",
    "\n",
    "# Creating a list of the gpdf to loop through and find the smallest lake size\n",
    "dfs = [water2014_clip, water2016_clip, water2018_clip]\n",
    "dfs_names = [\"water2014_clip\", \"water2016_clip\", \"water2018_clip\"]\n",
    "\n",
    "# New field for impairment status in all data sets\n",
    "for df in dfs:\n",
    "    df[\"status\"] = \"Impaired\"\n",
    "\n",
    "def find_min(dfs):\n",
    "    '''\n",
    "    finds smallest lake within the impaired datasets\n",
    "    Parameter: list of dataframes\n",
    "    '''\n",
    "    \n",
    "    global minimum\n",
    "    minimum = []\n",
    "    for df in dfs:\n",
    "        minimum.append(df[\"AREA_ACRES\"].min())\n",
    "    minimum = min(minimum)\n",
    "\n",
    "# Find smallest lake size of from all impaired lakes\n",
    "find_min(dfs)\n",
    "\n",
    "\n",
    "###\n",
    "### CLEANING THE HYDROGRAPHY DATA SET\n",
    "###\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "hydro_drop_invalid = hydrography.loc[hydrography['geometry'].is_valid, :]\n",
    "\n",
    "# Clipping hydro to the 7 county metro\n",
    "hydro_clip = gpd.clip(hydro_drop_invalid, metro_dissolve)\n",
    "\n",
    "# Narrowing down the number of features in the hydro layer to only lakes and ponds\n",
    "hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]\n",
    "\n",
    "# Selecting only the lakes that are at least the size of the the impaired water dataframes\n",
    "hydro_lake = hydro_lake.loc[(hydro_lake[\"acres\"] >= minimum)]\n",
    "\n",
    "# Dropping all excess fields from the dataframe\n",
    "hydro_clean = hydro_lake.drop([\"fw_id\", \"dowlknum\", \"sub_flag\", \"wb_class\", \"lake_class\", \"shore_mi\", \"center_utm\", \"center_u_1\",\n",
    "                               \"dnr_region\", \"fsh_office\", \"outside_mn\", \"delineated\", \"delineatio\", \"delineat_1\", \"delineat_2\", \n",
    "                               \"approved_b\", \"approval_d\", \"approval_n\", \"has_flag\", \"flag_type\", \"publish_da\", \"lksdb_basi\", \"has_wld_fl\",\n",
    "                               \"wld_flag_t\", \"created_us\", \"created_da\", \"last_edite\", \"last_edi_1\", \"ow_use\", \"pwi_class\", \"map_displa\", \n",
    "                               \"shape_Leng\", \"shape_Area\", \"INSIDE_X\", \"INSIDE_Y\", \"in_lakefin\"], axis = 1)\n",
    "\n",
    "# New field for impairment status to be used when data is joined with the impaired data sets\n",
    "hydro_clean[\"status\"] = \"\"\n",
    "\n",
    "# Dissolve hydrography geometry by lake name \n",
    "hydro_clean = hydro_clean.rename(columns={'pw_basin_n': 'NAME'})\n",
    "hydro_dis = (hydro_clean.dissolve(by='NAME')).reset_index()\n",
    "\n",
    "\n",
    "def complete_hydro(waterdata, waterdata_name):\n",
    "    '''\n",
    "    [add doc]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Combine nonimpaired with impaired. Returns a pandas dataframe\n",
    "    join_hydro = hydro_dis.merge(waterdata, on ='NAME', how='left') \n",
    "    \n",
    "    # Set geometry to hydro_dis dataset for all features\n",
    "    projected = join_hydro.set_geometry(join_hydro['geometry_x'], \n",
    "                                        crs='EPSG:26915')\n",
    "    \n",
    "    # Combine duplicate features\n",
    "    projected_dis = (projected.dissolve(by='NAME')).reset_index() \n",
    "    \n",
    "    # Remove unneccessary fields\n",
    "    projected_dis = projected_dis[['NAME', \n",
    "                                   'geometry', \n",
    "                                   'acres', \n",
    "                                   'cty_name', \n",
    "                                   'unique_id', \n",
    "                                   'status_y']]\n",
    "    \n",
    "    # Fill status of nonimpaired lakes\n",
    "    projected_dis = projected_dis.fillna(\"nonimpaired\")\n",
    "    projected_df = projected_dis.rename(columns = {'status_y': 'status'})\n",
    "\n",
    "    projected_df.to_file(f'{waterdata_name}.shp')\n",
    "\n",
    "# Nonimpaired and impaired completed dataset for each year    \n",
    "for df in dfs:\n",
    "    for name in dfs_names:\n",
    "        complete_hydro(df, name)\n",
    "\n",
    "    \n",
    "###\n",
    "### JOINING GEOMETRY TO WATER2020_CLEAN\n",
    "### \n",
    "\n",
    "water2020_join_auid = water2020_clean.merge(water2018_clip, how = \"left\", on = \"AUID\")\n",
    "\n",
    "water2020_join_auid = water2020_join_auid[[\"AUID\", \"NAME_x\", \"COUNTY_x\", \"AREA_ACRES\", \"geometry_y\", \"status\"]]\n",
    "\n",
    "water2020_join_auid = water2020_join_auid.rename(columns = {\"NAME_x\" : \"NAME\", \"COUNTY_x\" : \"COUNTY\", \"geometry_y\" : \"geometry\"})\n",
    "\n",
    "complete_hydro(water2020_join_auid, \"water2020_clip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "\n",
    "water2014_buffer = gpd.GeoDataFrame(water2014.buffer(500))\n",
    "water2014_buffer[\"NAME\"] = water2014[\"NAME\"]\n",
    "water2014_buffer = water2014_buffer.set_geometry(water2014_buffer[0])\n",
    "water2014_buffer = water2014_buffer[[\"NAME\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(input(\"Provide a distance for the size of the buffer in meters: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "water2016 = gpd.read_file(\"water2016_clip.shp\")\n",
    "water2018 = gpd.read_file(\"water2018_clip.shp\")\n",
    "water2020 = gpd.read_file(\"water2020_clip.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_lakes(buffer, water_feat):\n",
    "    lake_buffer = gpd.GeoDataFrame(water_feat.buffer(buffer))\n",
    "    lake_buffer[\"NAME\"] = water_feat[\"NAME\"]\n",
    "    lake_buffer = lake_buffer.set_geometry(lake_buffer[0])\n",
    "    lake_buffer = lake_buffer[[\"NAME\", \"geometry\"]]\n",
    "    return lake_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer2014 = buffer_lakes(buffer_size, water2014)\n",
    "buffer2016 = buffer_lakes(buffer_size, water2016)\n",
    "buffer2018 = buffer_lakes(buffer_size, water2018)\n",
    "buffer2020 = buffer_lakes(buffer_size, water2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all metro data by finding all files ending in _metro.zip\n",
    "directory = r'/home/leex6165/gisproj/'\n",
    "path = f'{directory}*19_metro.zip'\n",
    "\n",
    "buffer = buffer2018\n",
    "data_2018 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2018['status']})\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2018 = data_2018.merge(data_grp, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{directory}*20_metro.zip'\n",
    "\n",
    "buffer = buffer2020\n",
    "data_2020 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2020['status']} )\n",
    "\n",
    "# Get counts in each lake buffer per month\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2020 = data_2020.merge(data_grp, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKE 2 CELLS BELOW INTO FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of visitation for nonimpaired and impaired lakes 2020\n",
    "def vis_stats(counts_df):\n",
    "    vis = counts_df.groupby(['STATUS']).sum()\n",
    "    lkcounts = counts_df.groupby(['STATUS'])['NAME'].count()\n",
    "\n",
    "    vis['avg monthly vis'] = vis.mean(axis=1)\n",
    "    vis['Lake Counts'] = lkcounts\n",
    "    vis['Total visits'] = (vis.sum(axis=1))\n",
    "    vis['Total visits'] = vis['Total visits'] - (vis['avg monthly vis'] + vis['Lake Counts'])\n",
    "    vis['Avg monthly vis per lake'] = vis['avg monthly vis'] / vis['Lake Counts']\n",
    "    vis['Avg yearly vis per lake'] = vis['Total visits'] / vis['Lake Counts']\n",
    "    vis.loc['Total vis per month']= vis.sum(axis=0)\n",
    "\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_2020 = vis_stats(data_2020)\n",
    "vis_2020.to_csv(f'{directory}/vis_stats2020_{buffer_size}m.csv', \n",
    "                index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_2018 = vis_stats(data_2018)\n",
    "vis_2018.to_csv(f'{directory}/vis_stats2018_{buffer_size}m.csv', \n",
    "                index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(counts_df, year):\n",
    "    total = counts_df\n",
    "    total['Total visits'] = total.sum(axis=1)\n",
    "\n",
    "    maximum = total.sort_values(by=['Total visits'], ascending=False, ignore_index=True)\n",
    "    print(f'The top five most visted lakes for {year} are: ')\n",
    "    for row in range(len(maximum[0:5])):\n",
    "        print(f\"Lake Name: {maximum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {maximum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {maximum['STATUS'][row]}\\n\")\n",
    "\n",
    "    minimum = total.sort_values(by=['Total visits'], ascending=True, ignore_index=True)\n",
    "    print(f'The top five least visted lakes for {year} are: ')\n",
    "    for row in range(len(minimum[0:5])):\n",
    "        print(f\"Lake Name: {minimum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {minimum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {minimum['STATUS'][row]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(data_2020, \"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(data_2018, \"2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING ADDED AND REMOVED LAKES BETWEEN EACH YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impaired2014 = gpd.read_file(\"water2014_impaired.shp\")\n",
    "impaired2016 = gpd.read_file(\"water2016_impaired.shp\")\n",
    "impaired2018 = gpd.read_file(\"water2018_impaired.shp\")\n",
    "\n",
    "impaired2020 = pd.read_csv(\"water2020_impaired.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_16 = impaired2016.loc[impaired2016[\"AUID\"].isin(impaired2014[\"AUID\"]) == False]\n",
    "\n",
    "removed_14 = impaired2014.loc[impaired2014[\"AUID\"].isin(impaired2016[\"AUID\"]) == False]\n",
    "\n",
    "added_18 = impaired2018.loc[impaired2018[\"AUID\"].isin(impaired2016[\"AUID\"]) == False]\n",
    "\n",
    "removed_16 = impaired2016.loc[impaired2016[\"AUID\"].isin(impaired2018[\"AUID\"]) == False]\n",
    "\n",
    "added_20 = impaired2020.loc[impaired2020[\"AUID\"].isin(impaired2018[\"AUID\"]) == False]\n",
    "\n",
    "removed_18 = impaired2018.loc[impaired2018[\"AUID\"].isin(impaired2020[\"AUID\"]) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impaired_change(y1_df, y1, y2_df, y2):\n",
    "    added = y2_df.loc[y2_df['AUID'].isin(y1_df['AUID']) == False]\n",
    "    removed = y1_df.loc[y1_df['AUID'].isin(y2_df['AUID']) == False]\n",
    "    \n",
    "    print(f\"There were {len(removed['NAME'])} lakes removed from impaired waters list {y1}-{y2}:\")\n",
    "    for row in removed['NAME']:\n",
    "        print (row)\n",
    "    \n",
    "    print(f\"\\nThere were {len(added['NAME'])} lakes added to impaired waters list {y1}-{y2}:\")\n",
    "    for row in added['NAME']:\n",
    "        print(row)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impaired_change(impaired2014, '2014', impaired2016, '2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From the years 2014-2016 the following lakes were removed from the impaired waters list:\")\n",
    "for row in removed_14[\"NAME\"]:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nFrom the years 2014-2016 the following lakes were added to the impaired waters list:\")\n",
    "for row in added_16[\"NAME\"]:\n",
    "    print(row)\n",
    "    \n",
    "print(\"\\nFrom the years 2016-2018 the following lakes were removed from the impaired waters list:\")\n",
    "for row in removed_16[\"NAME\"]:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nFrom the years 2016-2018 the following lakes were added to the impaired waters list:\")\n",
    "for row in added_18[\"NAME\"]:\n",
    "    print(row)\n",
    "    \n",
    "print(\"\\nFrom the years 2018-2020 the following lakes were removed from the impaired waters list:\")\n",
    "for row in removed_18[\"NAME\"]:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nFrom the years 2018-2020 the following lakes were added to the impaired waters list:\")\n",
    "for row in added_20[\"NAME\"]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add20 = added_20.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_18.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Nicole Dunn and Maisong Francis\n",
    "\n",
    "This script is designed to be used after WaterData.py and SafeGraph.py.\n",
    "This script buffers lake features based on user input and gets visitation counts\n",
    "within each lake buffer. The results are statistical outputs of visitation \n",
    "counts per month and year for each category of impairment status: impaired and \n",
    "nonimpaired, and returns top five most and least visited lakes for each year. \n",
    "'''\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import glob\n",
    "\n",
    "# Load in all the clipped shapefiles\n",
    "water2014 = gpd.read_file(\"water2014_clip.shp\")\n",
    "water2016 = gpd.read_file(\"water2016_clip.shp\")\n",
    "water2018 = gpd.read_file(\"water2018_clip.shp\")\n",
    "water2020 = gpd.read_file(\"water2020_clip.shp\")\n",
    "\n",
    "# Load in impaired only clipped files\n",
    "impaired2014 = gpd.read_file(\"water2014_impaired.shp\")\n",
    "impaired2016 = gpd.read_file(\"water2016_impaired.shp\")\n",
    "impaired2018 = gpd.read_file(\"water2018_impaired.shp\")\n",
    "impaired2020 = pd.read_csv(\"water2020_impaired.csv\")\n",
    "\n",
    "def buffer_lakes(buffer, water_feat):\n",
    "    ''' Buffer the clipped impaired water features.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    buffer: int\n",
    "        The user obtained distance for the buffer function\n",
    "    water_feat: geodataframe\n",
    "        The geodataframe of an impaired water dataset\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    lake_buffer: geodataframe\n",
    "        The buffer around each lake feature in the impaired water dataframe\n",
    "    '''\n",
    "    lake_buffer = gpd.GeoDataFrame(water_feat.buffer(buffer))\n",
    "    lake_buffer[\"NAME\"] = water_feat[\"NAME\"]\n",
    "    lake_buffer = lake_buffer.set_geometry(lake_buffer[0])\n",
    "    lake_buffer = lake_buffer[[\"NAME\", \"geometry\"]]\n",
    "    return lake_buffer\n",
    "\n",
    "\n",
    "def vis_stats(counts_df):\n",
    "    ''' Compute statistical metrics for visitations to impaired and nonimpaired \n",
    "    lakes. Metrics include average month visits, lake counts, total visits, \n",
    "    averae month visits per lake, average yearly visits per lake, and total visits\n",
    "    per year. \n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    counts_df: gpd DataFrame\n",
    "        The dataframe produced from spatial joining visitation counts to lake \n",
    "        buffers.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    vis\n",
    "        gpd dataframe with statistical metrics\n",
    "    '''\n",
    "    \n",
    "    vis = counts_df.groupby(['STATUS']).sum()\n",
    "    lkcounts = counts_df.groupby(['STATUS'])['NAME'].count()\n",
    "\n",
    "    vis['avg monthly vis'] = vis.mean(axis=1)\n",
    "    vis['Lake Counts'] = lkcounts\n",
    "    vis['Total visits'] = (vis.sum(axis=1))\n",
    "    vis['Total visits'] = vis['Total visits'] - (vis['avg monthly vis'] + vis['Lake Counts'])\n",
    "    vis['Avg monthly vis per lake'] = vis['avg monthly vis'] / vis['Lake Counts']\n",
    "    vis['Avg yearly vis per lake'] = vis['Total visits'] / vis['Lake Counts']\n",
    "    vis.loc['Total vis per year']= vis.sum(axis=0)\n",
    "    return vis\n",
    "\n",
    "\n",
    "def min_max(counts_df, year):\n",
    "    '''\n",
    "    Returns most visited and least visted lakes with impairment status and \n",
    "    visitation counts. \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    counts_df: gpd DataFrame\n",
    "       The dataframe produced from spatial joining visitation counts to lake \n",
    "       buffers.\n",
    "    year: str\n",
    "       The year of the impaired waters dataset\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "       Printed names, impairment status, and visitation counts of top five most\n",
    "       and least visited lakes.\n",
    "    '''\n",
    "    total = counts_df\n",
    "    total['Total visits'] = total.sum(axis=1)\n",
    "\n",
    "    maximum = total.sort_values(by=['Total visits'], \n",
    "                                ascending=False, \n",
    "                                ignore_index=True)\n",
    "    print(f'The top five most visted lakes for {year} are: ')\n",
    "    for row in range(len(maximum[0:5])):\n",
    "        print(f\"Lake Name: {maximum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {maximum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {maximum['STATUS'][row]}\\n\")\n",
    "\n",
    "    minimum = total.sort_values(by=['Total visits'], \n",
    "                                ascending=True, \n",
    "                                ignore_index=True)\n",
    "    \n",
    "    print(f'The top five least visted lakes for {year} are: ')\n",
    "    for row in range(len(minimum[0:5])):\n",
    "        print(f\"Lake Name: {minimum['NAME'][row]}\"\n",
    "              f\"\\nTotal visits: {minimum['Total visits'][row]}\"\n",
    "              f\"\\nStatus: {minimum['STATUS'][row]}\\n\")\n",
    "\n",
    "    \n",
    "def impaired_change(y1_df, y1, y2_df, y2):\n",
    "    ''' Returns removed and added impaired lakes between two biennial \n",
    "    impaired waters lists. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y1_df: gpd DataFrame\n",
    "        The impaired waters clipped to study area and written out from \n",
    "        WaterData.py and read with Geopandas.Must be the earlier year between \n",
    "        two dataframes being compared. \n",
    "    y1: str\n",
    "        Year of earlier dataset\n",
    "    y2_df:\n",
    "        The impaired waters clipped to study area and written out from \n",
    "        WaterData.py and read with Geopandas. Must be the later year between \n",
    "        two dataframes being compared. \n",
    "    y2: str\n",
    "        Year of the later dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The number of impaired lakes removed and added, and the names of those lakes. \n",
    "    '''\n",
    "    \n",
    "    added = y2_df.loc[y2_df['AUID'].isin(y1_df['AUID']) == False]\n",
    "    removed = y1_df.loc[y1_df['AUID'].isin(y2_df['AUID']) == False]\n",
    "    \n",
    "    print(f\"There were {len(removed['NAME'])} lakes removed from impaired waters list {y1}-{y2}:\")\n",
    "    for row in removed['NAME']:\n",
    "        print (row)\n",
    "    \n",
    "    print(f\"\\nThere were {len(added['NAME'])} lakes added to impaired waters list {y1}-{y2}:\")\n",
    "    for row in added['NAME']:\n",
    "        print(row)             \n",
    "        \n",
    "##############################################################################\n",
    "\n",
    "# Get a user input for the size of the buffer\n",
    "buffer_size = int(input(\"Provide a distance for the size of the buffer in meters: \"))\n",
    "              \n",
    "# Calling the buffer_lakes function for each year of the impaired \n",
    "# water datasets and assigning them to new variables\n",
    "buffer2014 = buffer_lakes(buffer_size, water2014)\n",
    "buffer2016 = buffer_lakes(buffer_size, water2016)\n",
    "buffer2018 = buffer_lakes(buffer_size, water2018)\n",
    "buffer2020 = buffer_lakes(buffer_size, water2020)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "# Get all metro data by finding all files ending in _metro.zip\n",
    "directory = r'Project/'\n",
    "path = f'{directory}*19_metro.zip'\n",
    "\n",
    "buffer = buffer2018\n",
    "data_2018 = pd.DataFrame({'NAME': buffer['NAME'], 'STATUS': water2018['status']})\n",
    "\n",
    "# Get counts in each lake buffer per month for 2018 water data and 2019 foot traffic\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2018 = data_2018.merge(data_grp, how='outer')\n",
    "    \n",
    "    \n",
    "##############################################################################    \n",
    "    \n",
    "# Get counts in each lake buffer per month for 2018 water data and 2019 foot traffic\n",
    "path = f'{directory}*20_metro.zip'\n",
    "buffer = buffer2020\n",
    "data_2020 = pd.DataFrame({'NAME': buffer['NAME'], \n",
    "                          'STATUS': water2020['status']} )\n",
    "\n",
    "for file in glob.glob(path):\n",
    "    sg_data = f'zip://{file}'\n",
    "    patterns = (gpd.read_file(sg_data)).to_crs('EPSG:26915')\n",
    "    data_join = gpd.sjoin(buffer, patterns, op='intersects')\n",
    "\n",
    "    # Get counts of points in each lake buffer.\n",
    "    data_grp = data_join.groupby('NAME', as_index=False)['index_right'].count()\n",
    "    data_grp = data_grp.rename(columns = {'index_right': f'{file[-15:-10]}_counts'})\n",
    "    \n",
    "    data_2020 = data_2020.merge(data_grp, how='outer')\n",
    "    \n",
    "############################################################################## \n",
    "\n",
    "# Counts of visitation for nonimpaired and impaired lakes for each year\n",
    "# Write results to csv\n",
    "vis_2018 = vis_stats(data_2018)\n",
    "vis_2018.to_csv(f'{directory}/vis_stats2018_{buffer_size}m.csv', \n",
    "                index=True)\n",
    "\n",
    "vis_2020 = vis_stats(data_2020)\n",
    "vis_2020.to_csv(f'{directory}/vis_stats2020_{buffer_size}m.csv', \n",
    "                index=True)\n",
    "\n",
    "\n",
    "# Find most and least visited lake for each year\n",
    "min_max(data_2018, \"2018\")\n",
    "min_max(data_2020, \"2020\")\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "### FINDING ADDED AND REMOVED LAKES BETWEEN EACH YEAR\n",
    "\n",
    "impaired_change(impaired2014, '2014', impaired2016, '2016')\n",
    "impaired_change(impaired2016, '2016', impaired2018, '2018')\n",
    "impaired_change(impaired2018, '2018', impaired2020, '2020')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
