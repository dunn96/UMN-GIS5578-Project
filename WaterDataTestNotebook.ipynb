{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load hydrography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrography = gpd.read_file(\"zip://shp_water_dnr_hydrography.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrography.head()\n",
    "for col in hydrography.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hydrography.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate all invalid gometries and drop them from the dataset\n",
    "hydro_drop_invalid = hydrography.loc[hydrography['geometry'].is_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in hydro_drop_invalid.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 7 county metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro = gpd.read_file(\"zip://shp_bdry_metro_counties_and_ctus.zip\")\n",
    "metro.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_dissolve = metro.dissolve(by = \"CO_NAME\")\n",
    "metro_dissolve.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_dissolve.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip hydro to metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varifying the projections\n",
    "print(metro_dissolve.crs)\n",
    "print(hydrography.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for invalid polygon geometries since the clip operation did not work\n",
    "for i in hydrography.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/63955752/topologicalerror-the-operation-geosintersection-r-could-not-be-performed\n",
    "hydro_valid = hydrography.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for invalid polygon geometries since the clip operation did not work\n",
    "for i in hydro_valid.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clip = gpd.clip(hydro_drop_invalid, metro_dissolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_lake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clip.head()\n",
    "\n",
    "#hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]\n",
    "#hydro_lake.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 2014, 2016, 2018 water files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018 = gpd.read_file(\"zip://impaired_2018_lakes.zip\")\n",
    "water2016 = gpd.read_file(\"zip://impaired_2016_lakes.zip\")\n",
    "water2014 = gpd.read_file(\"zip://impaired_2014_lakes.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water2018.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018 = water2018.drop([\"CAT\", \"CAT_DESC\", \"REACH_DESC\", \"USE_CLASS\", \"AREA_ACRES\", \"AFFECTED_U\", \"LIKE_MEET\", \"NON_POLL\", \n",
    "                            \"NAT_BACK\", \"ADD_MON\", \"APPROVED\", \"NEEDS_PLN\", \"IMP_PARAM\", \"NEW_IMPAIR\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \n",
    "                            \"TRIBAL_INT\", \"INDIAN_RES\", \"AMMONIA\", \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \n",
    "                            \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"Shape_Leng\", \"Shape_Area\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water2016.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016 = water2016.drop([\"CAT\", \"DATASET_NA\", \"REACH_DESC\", \"USE_CLASS\", \"AREA_ACRES\", \"AFFECTED_U\", \"TMDL_NOT_R\", \"TMDL_NOT_1\", \"IMPAIR_PAR\", \"IMPAIR_P_1\", \"NEW_IMPAIR\", \"NEW_IMPA_1\", \n",
    "                \"TMDL_APPRO\", \"TMDL_APP_1\", \"TMDL_NEEDE\", \"TMDL_NEE_1\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \n",
    "                \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \n",
    "                \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"SHAPE_Leng\", \"SHAPE_Area\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water2014.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = water2014.drop([\"LOCATION\", \"ACRES\", \"CAT\", \"AFFECTED_U\", \"NOPLN\", \"APPROVED\", \"NEEDSPLN\", \"IMPAIR_PAR\", \"NEW_2014\", \n",
    "                \"HUC8\", \"HUC8_NAME\", \"HUC4\", \"BASIN\", \"WDWMO_NAME\", \"WDWMO_TYPE\", \"Chloride\", \n",
    "                \"HgF\", \"HgW\", \"Nutrients\", \"PCBF\", \"PFOS_W\", \"SHAPE_Leng\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
    "water2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014 = water2014.rename(columns = {\"WATER_NAME\" : \"NAME\", \"ALL_COUNTI\" : \"COUNTY\"})\n",
    "water2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipping impaired waters layers to 7 county metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for invalid polygon geometries since the clip operation did not work\n",
    "for i in water2018.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_drop_invalid = water2018.loc[water2018['geometry'].is_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in water2018_drop_invalid.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_drop_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in water2016.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016_drop_invalid = water2016.loc[water2016['geometry'].is_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in water2016_drop_invalid.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016_drop_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in water2014.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_drop_invalid = water2014.loc[water2014['geometry'].is_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in water2014_drop_invalid.is_valid:\n",
    "    if i == False:\n",
    "        print(\"poly is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_drop_invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_clip = gpd.clip(water2018_drop_invalid, metro_dissolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016_clip = gpd.clip(water2016_drop_invalid, metro_dissolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_proj = water2014_drop_invalid.to_crs('EPSG:26915')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_clip = gpd.clip(water2014_proj, metro_dissolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_clip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load water 2020 data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in water2020.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"Water body type\", \"geometry\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_lake = water2020.loc[(water2020[\"Water body type\"] == \"Lake\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_lake = water2020_lake[[\"Water body name\", \"AUID\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to match the two other datasets\n",
    "water2020_lake = water2020_lake.rename(columns = {\"Water body name\" : \"NAME\"})\n",
    "\n",
    "water2020_lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join test2 to water2018\n",
    "# output 1946 rows...there are duplicates...???\n",
    "jointest = test2.merge(water2018, on = \"AUID\")\n",
    "\n",
    "jointest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test2.groupby(\"AUID\", as_index = False)[\"Pollutant or stressor\"].apply(\";\".join)\n",
    "test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join test3 back to test2\n",
    "\n",
    "test4 = test2.merge(test3, how = \"inner\", on = \"AUID\")\n",
    "test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the draft 2020 impaired water list\n",
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n",
    "\n",
    "# retreiving all the column names\n",
    "for col in water2020.columns:\n",
    "    print(col)\n",
    "\n",
    "# Sampling the data, to get a look at what we are dealing with\n",
    "water2020.head()\n",
    "\n",
    "# Selecting out the columns we want to keep\n",
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"Water body type\", \"Use Class\", \"Pollutant or stressor\", \"geometry\"]]\n",
    "water2020.head()\n",
    "\n",
    "# Selecting out only the lake features to keep in line with the 3 other shapefiles we are already working with\n",
    "water2020_lake = water2020.loc[(test[\"Water body type\"] == \"Lake\")]\n",
    "\n",
    "# Removing duplicate records --> grouping by AUID, and joining the polluatant or stressor field with \";\" when they differ\n",
    "# for the repeated AUID. This happens because a lake can have more than one stressor to get on the impaired water list.\n",
    "water2020_remove_duplicates = water2020_lake.groupby(\"AUID\", as_index = False)[\"Pollutant or stressor\"].apply(\";\".join)\n",
    "water2020_remove_duplicates\n",
    "\n",
    "# Failed attempt to join the other needed columns back to the dataset after removing the duplicates\n",
    "jointest1 = water2020_remove_duplicates.merge(water2020_lake, how = \"inner\", on = \"AUID\")\n",
    "jointest1\n",
    "\n",
    "# Attempt to get geometry for the 2020 dataset from the 2018 dataset\n",
    "# Have not figured out the best way to do this yet. \n",
    "jointest2 = water2020_remove_duplicates.merge(water2018, on = \"AUID\")\n",
    "jointest2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load blockgroup data and intersecting it with the water data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups_df = gpd.read_file('zip://tl_2019_27_bg.zip')\n",
    "print(f'Loaded {len(blockgroups_df):,} block groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(blockgroups_df.crs)\n",
    "print(water2018.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_proj = blockgroups_df.to_crs('EPSG:26915')\n",
    "bg_proj.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_intersect_bg_proj = gpd.overlay(water2018, bg_proj, how='intersection')\n",
    "water2018_intersect_bg_proj.plot()\n",
    "water2018_intersect_bg_proj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Review Comments ###\n",
    "# add columns and then remove duplicates again\n",
    "# find lakes that have been added/removed, and look at the traffic at that spot now\n",
    "# Smaller scale? Just one city\n",
    "# function for selecting data with block groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Code so far (11/6/2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Nicole Dunn\n",
    "\n",
    "This python script purpose is to clean spatial data files that will be used in \n",
    "future analysis. Cleaning the data includes removing fields that are not needed, \n",
    "removing invalid geometries from the geodataframes, adding geometery where there \n",
    "is none, and clipping all the data to the 7 county metro as the area of interest.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Loading all shapefile datasets in as geopandas dataframes\n",
    "hydrography = gpd.read_file(\"zip://shp_water_dnr_hydrography.zip\")\n",
    "water2018 = gpd.read_file(\"zip://impaired_2018_lakes.zip\")\n",
    "water2016 = gpd.read_file(\"zip://impaired_2016_lakes.zip\")\n",
    "water2014 = gpd.read_file(\"zip://impaired_2014_lakes.zip\")\n",
    "metro = gpd.read_file(\"zip://shp_bdry_metro_counties_and_ctus.zip\")\n",
    "\n",
    "\n",
    "# Cleaning the metro dataset, dissolving on the county name. \n",
    "metro_dissolve = metro.dissolve(by = \"CO_NAME\")\n",
    "\n",
    "# There are many columns in this dataset,\n",
    "# this is to be able to see all of them to easily\n",
    "# choose the ones we will keep\n",
    "columns18 =[]\n",
    "columns16 =[]\n",
    "columns14 =[]\n",
    "\n",
    "for col in water2018.columns:\n",
    "    columns18.append(col)\n",
    "    \n",
    "for col in water2016.columns:\n",
    "    columns16.append(col)\n",
    "    \n",
    "for col in water2014.columns:\n",
    "    columns14.append(col)\n",
    "\n",
    "# Dropping all the unnecessary columns\n",
    "water2018 = water2018.drop([\"CAT\", \"CAT_DESC\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"LIKE_MEET\", \n",
    "                            \"NON_POLL\", \"NAT_BACK\", \"ADD_MON\", \"APPROVED\", \"NEEDS_PLN\", \"IMP_PARAM\", \"NEW_IMPAIR\", \n",
    "                            \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \"AMMONIA\", \"CHLORIDE\", \n",
    "                            \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"Shape_Leng\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "water2016 = water2016.drop([\"CAT\", \"DATASET_NA\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"TMDL_NOT_R\", \n",
    "                            \"TMDL_NOT_1\", \"IMPAIR_PAR\", \"IMPAIR_P_1\", \"NEW_IMPAIR\", \"NEW_IMPA_1\", \"TMDL_APPRO\", \"TMDL_APP_1\", \n",
    "                            \"TMDL_NEEDE\", \"TMDL_NEE_1\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \n",
    "                            \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"SHAPE_Leng\", \"SHAPE_Area\"], axis = 1)\n",
    "\n",
    "water2014 = water2014.drop([\"LOCATION\", \"CAT\", \"AFFECTED_U\", \"NOPLN\", \"APPROVED\", \"NEEDSPLN\", \"IMPAIR_PAR\", \n",
    "                            \"NEW_2014\", \"HUC8\", \"HUC8_NAME\", \"HUC4\", \"BASIN\", \"WDWMO_NAME\", \"WDWMO_TYPE\", \"Chloride\", \n",
    "                            \"HgF\", \"HgW\", \"Nutrients\", \"PCBF\", \"PFOS_W\", \"SHAPE_Leng\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2014 = water2014.rename(columns = {\"WATER_NAME\" : \"NAME\", \"ALL_COUNTI\" : \"COUNTY\", \"ACRES\" : \"AREA_ACRES\"})\n",
    "\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "water2018_drop_invalid = water2018.loc[water2018['geometry'].is_valid, :]\n",
    "\n",
    "water2016_drop_invalid = water2016.loc[water2016['geometry'].is_valid, :]\n",
    "\n",
    "water2014_drop_invalid = water2014.loc[water2014['geometry'].is_valid, :]\n",
    "\n",
    "\n",
    "# Clipping the three impaired water files to the 7 county metro\n",
    "water2018_clip = gpd.clip(water2018_drop_invalid, metro_dissolve)\n",
    "water2016_clip = gpd.clip(water2016_drop_invalid, metro_dissolve)\n",
    "\n",
    "# 2014 needed to be reprojected - then clip was performed\n",
    "water2014_proj = water2014_drop_invalid.to_crs('EPSG:26915')\n",
    "water2014_clip = gpd.clip(water2014_proj, metro_dissolve)\n",
    "\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "hydro_drop_invalid = hydrography.loc[hydrography['geometry'].is_valid, :]\n",
    "\n",
    "# Clipping hydro to the 7 county metro\n",
    "hydro_clip = gpd.clip(hydro_drop_invalid, metro_dissolve)\n",
    "\n",
    "# Narrowing down the number of features in the hydro layer to only lakes and ponds\n",
    "hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]\n",
    "\n",
    "\n",
    "# Load water 2020 data csv, selecting out the columns that we want and addinga geometry column\n",
    "# and pulling out only the lake features.\n",
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n",
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"County\", \"Water body type\", \"geometry\"]]\n",
    "water2020_lake = water2020.loc[(water2020[\"Water body type\"] == \"Lake\")]\n",
    "\n",
    "# Dropping the \"water body type\" field since it is no longer needed\n",
    "water2020_lake = water2020_lake[[\"AUID\", \"Water body name\", \"County\", \"geometry\"]]\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2020_lake = water2020_lake.rename(columns = {\"Water body name\" : \"NAME\", \"County\" : \"COUNTY\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_lake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = [\"Anoka\", \"Hennepin\", \"Ramsey\", \"Washington\", \"Carver\", \"Scott\", \"Dakota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_lake.loc[(water2020_lake[\"COUNTY\"].isin(counties))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro[\"COUNTY\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Anoka\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Hennepin\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro= water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Ramsey\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Washington\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Carver\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Scott\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro = water2020_metro.append(water2020_lake.loc[(water2020_lake[\"COUNTY\"] == \"Dakota\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_clean = water2020_metro.drop_duplicates(subset = [\"AUID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2014_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2016_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_clip[\"AREA_ACRES\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [water2014_clip, water2016_clip, water2018_clip]\n",
    "\n",
    "for df in dfs:\n",
    "    #print(df)\n",
    "    #print(type(df))\n",
    "    smallest_lake = df[\"AREA_ACRES\"].min()\n",
    "    minimum = 0\n",
    "    if smallest_lake > minimum:\n",
    "        minimum = smallest_lake\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [water2014_clip, water2016_clip, water2018_clip]\n",
    "\n",
    "def find_min(dfs):\n",
    "    for df in dfs:\n",
    "        smallest_lake = df[\"AREA_ACRES\"].min()\n",
    "        minimum = 0\n",
    "        if smallest_lake > minimum:\n",
    "            minimum = smallest_lake\n",
    "        else:\n",
    "            pass\n",
    "    return minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_min(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df[\"status\"] = \"Impaired\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2018_clip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_lake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_lake = hydro_lake.loc[(hydro_lake[\"acres\"] >= minimum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in hydro_lake.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clean = hydro_lake.drop([\"fw_id\", \"dowlknum\", \"sub_flag\", \"wb_class\", \"lake_class\", \"shore_mi\", \"center_utm\", \"center_u_1\",\n",
    "                               \"dnr_region\", \"fsh_office\", \"outside_mn\", \"delineated\", \"delineatio\", \"delineat_1\", \"delineat_2\", \n",
    "                               \"approved_b\", \"approval_d\", \"approval_n\", \"has_flag\", \"flag_type\", \"publish_da\", \"lksdb_basi\", \"has_wld_fl\",\n",
    "                               \"wld_flag_t\", \"created_us\", \"created_da\", \"last_edite\", \"last_edi_1\", \"ow_use\", \"pwi_class\", \"map_displa\", \n",
    "                               \"shape_Leng\", \"shape_Area\", \"INSIDE_X\", \"INSIDE_Y\", \"in_lakefin\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clean[\"status\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code as of 11/16/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Nicole Dunn\n",
    "\n",
    "This python script purpose is to clean spatial data files that will be used in \n",
    "future analysis. Cleaning the data includes removing fields that are not needed, \n",
    "removing invalid geometries from the geodataframes, adding geometery where there \n",
    "is none, and clipping all the data to the 7 county metro as the area of interest.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Loading all shapefile datasets in as geopandas dataframes\n",
    "hydrography = gpd.read_file(\"zip://shp_water_dnr_hydrography.zip\")\n",
    "water2018 = gpd.read_file(\"zip://impaired_2018_lakes.zip\")\n",
    "water2016 = gpd.read_file(\"zip://impaired_2016_lakes.zip\")\n",
    "water2014 = gpd.read_file(\"zip://impaired_2014_lakes.zip\")\n",
    "metro = gpd.read_file(\"zip://shp_bdry_metro_counties_and_ctus.zip\")\n",
    "\n",
    "###\n",
    "### CLEANING AND CLIPPING IMPAIRED WATER 2014, 2016, AND 2018\n",
    "###\n",
    "\n",
    "# Cleaning the metro dataset, dissolving on the county name. \n",
    "metro_dissolve = metro.dissolve(by = \"CO_NAME\")\n",
    "\n",
    "# Dropping all the unnecessary columns\n",
    "water2018 = water2018.drop([\"CAT\", \"CAT_DESC\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"LIKE_MEET\", \n",
    "                            \"NON_POLL\", \"NAT_BACK\", \"ADD_MON\", \"APPROVED\", \"NEEDS_PLN\", \"IMP_PARAM\", \"NEW_IMPAIR\", \n",
    "                            \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \"AMMONIA\", \"CHLORIDE\", \n",
    "                            \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"Shape_Leng\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "water2016 = water2016.drop([\"CAT\", \"DATASET_NA\", \"REACH_DESC\", \"USE_CLASS\", \"AFFECTED_U\", \"TMDL_NOT_R\", \n",
    "                            \"TMDL_NOT_1\", \"IMPAIR_PAR\", \"IMPAIR_P_1\", \"NEW_IMPAIR\", \"NEW_IMPA_1\", \"TMDL_APPRO\", \"TMDL_APP_1\", \n",
    "                            \"TMDL_NEEDE\", \"TMDL_NEE_1\", \"HUC_8\", \"HUC_8_NAME\", \"HUC_4\", \"BASIN\", \"TRIBAL_INT\", \"INDIAN_RES\", \n",
    "                            \"CHLORIDE\", \"FISHESBIO\", \"HG_F\", \"HG_W\", \"NUTRIENTS\", \"PCB_F\", \"PFOS_F\", \"SHAPE_Leng\", \"SHAPE_Area\"], axis = 1)\n",
    "\n",
    "water2014 = water2014.drop([\"LOCATION\", \"CAT\", \"AFFECTED_U\", \"NOPLN\", \"APPROVED\", \"NEEDSPLN\", \"IMPAIR_PAR\", \n",
    "                            \"NEW_2014\", \"HUC8\", \"HUC8_NAME\", \"HUC4\", \"BASIN\", \"WDWMO_NAME\", \"WDWMO_TYPE\", \"Chloride\", \n",
    "                            \"HgF\", \"HgW\", \"Nutrients\", \"PCBF\", \"PFOS_W\", \"SHAPE_Leng\", \"Shape_Le_1\", \"Shape_Area\"], axis = 1)\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2014 = water2014.rename(columns = {\"WATER_NAME\" : \"NAME\", \"ALL_COUNTI\" : \"COUNTY\", \"ACRES\" : \"AREA_ACRES\"})\n",
    "\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "water2018_drop_invalid = water2018.loc[water2018['geometry'].is_valid, :]\n",
    "\n",
    "water2016_drop_invalid = water2016.loc[water2016['geometry'].is_valid, :]\n",
    "\n",
    "water2014_drop_invalid = water2014.loc[water2014['geometry'].is_valid, :]\n",
    "\n",
    "\n",
    "# Clipping the three impaired water files to the 7 county metro\n",
    "water2018_clip = gpd.clip(water2018_drop_invalid, metro_dissolve)\n",
    "water2016_clip = gpd.clip(water2016_drop_invalid, metro_dissolve)\n",
    "\n",
    "# 2014 needed to be reprojected - then clip was performed\n",
    "water2014_proj = water2014_drop_invalid.to_crs('EPSG:26915')\n",
    "water2014_clip = gpd.clip(water2014_proj, metro_dissolve)\n",
    "\n",
    "\n",
    "# Creating a list of the gpdf to loop through and find the smallest lake size\n",
    "dfs = [water2014_clip, water2016_clip, water2018_clip]\n",
    "\n",
    "# New field for impairment status in all data sets\n",
    "for df in dfs:\n",
    "    df[\"status\"] = \"Impaired\"\n",
    "\n",
    "def find_min(dfs):\n",
    "    '''\n",
    "    finds smallest lake within the impaired datasets\n",
    "    Parameter: list of dataframes\n",
    "    '''\n",
    "    global minimum\n",
    "    minimum = []\n",
    "    for df in dfs:\n",
    "        minimum.append(df[\"AREA_ACRES\"].min())\n",
    "    minimum = min(minimum)\n",
    "\n",
    "# Calling function\n",
    "find_min(dfs)\n",
    "\n",
    "###\n",
    "### CLEANING THE HYDROGRAPHY DATA SET\n",
    "###\n",
    "\n",
    "# Locate all invalid gometries and drop them from the dataset\n",
    "hydro_drop_invalid = hydrography.loc[hydrography['geometry'].is_valid, :]\n",
    "\n",
    "# Clipping hydro to the 7 county metro\n",
    "hydro_clip = gpd.clip(hydro_drop_invalid, metro_dissolve)\n",
    "\n",
    "# Narrowing down the number of features in the hydro layer to only lakes and ponds\n",
    "hydro_lake = hydro_clip.loc[hydro_clip[\"wb_class\"] == \"Lake or Pond\"]\n",
    "\n",
    "# Selecting only the lakes that are at least the size of the the impaired water dataframes\n",
    "hydro_lake = hydro_lake.loc[(hydro_lake[\"acres\"] >= minimum)]\n",
    "\n",
    "# Dropping all excess fields from the dataframe\n",
    "hydro_clean = hydro_lake.drop([\"fw_id\", \"dowlknum\", \"sub_flag\", \"wb_class\", \"lake_class\", \"shore_mi\", \"center_utm\", \"center_u_1\",\n",
    "                               \"dnr_region\", \"fsh_office\", \"outside_mn\", \"delineated\", \"delineatio\", \"delineat_1\", \"delineat_2\", \n",
    "                               \"approved_b\", \"approval_d\", \"approval_n\", \"has_flag\", \"flag_type\", \"publish_da\", \"lksdb_basi\", \"has_wld_fl\",\n",
    "                               \"wld_flag_t\", \"created_us\", \"created_da\", \"last_edite\", \"last_edi_1\", \"ow_use\", \"pwi_class\", \"map_displa\", \n",
    "                               \"shape_Leng\", \"shape_Area\", \"INSIDE_X\", \"INSIDE_Y\", \"in_lakefin\"], axis = 1)\n",
    "\n",
    "# New field for impairment status to be used when data is joined with the imparied data sets\n",
    "hydro_clean[\"status\"] = \"\"\n",
    "\n",
    "###\n",
    "### CLEANING THE 2020 IMPAIRED WATER DATA SET\n",
    "###\n",
    "\n",
    "# Load water 2020 data csv, selecting out the columns that we want and adding a geometry column\n",
    "# and pulling out only the lake features.\n",
    "water2020 = gpd.read_file(\"wq-iw1-65.csv\")\n",
    "water2020 = water2020[[\"Water body name\", \"AUID\", \"County\", \"Water body type\", \"geometry\"]]\n",
    "water2020_lake = water2020.loc[(water2020[\"Water body type\"] == \"Lake\")]\n",
    "\n",
    "# Dropping the \"water body type\" field since it is no longer needed\n",
    "water2020_lake = water2020_lake[[\"AUID\", \"Water body name\", \"County\", \"geometry\"]]\n",
    "\n",
    "# Renaming the columns to match the two other datasets\n",
    "water2020_lake = water2020_lake.rename(columns = {\"Water body name\" : \"NAME\", \"County\" : \"COUNTY\"})\n",
    "\n",
    "# Selecting out the 7 county metro\n",
    "counties = [\"Anoka\", \"Hennepin\", \"Ramsey\", \"Washington\", \"Carver\", \"Scott\", \"Dakota\"]\n",
    "water2020_metro = water2020_lake.loc[(water2020_lake[\"COUNTY\"].isin(counties))]\n",
    "\n",
    "# Varifying all the correct counties are there\n",
    "water2020_metro[\"COUNTY\"].unique()\n",
    "\n",
    "# Drop Duplicate AUIDs\n",
    "water2020_clean = water2020_metro.drop_duplicates(subset = [\"AUID\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched2020 = water2020_clean.loc[water2020_clean['AUID'].isin(water2018_clip['AUID']) == False]\n",
    "unmatched2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = unmatched2020.loc[unmatched2020[\"NAME\"].isin(hydro_clean[\"pw_basin_n\"]) == False]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water2020_join_auid = water2020_clean.merge(water2018_clip, how = \"left\", on = \"AUID\")\n",
    "water2020_join_auid\n",
    "\n",
    "hydro_clean = hydro_clean.rename(columns = {\"pw_basin_n\" : \"NAME_x\"})\n",
    "\n",
    "hydro_geometry = hydro_clean.loc[hydro_clean[\"NAME_x\"] == \"DeMontreville\"]\n",
    "\n",
    "hydro_geometry = hydro_geometry[[\"NAME_x\", \"geometry\", \"status\"]]\n",
    "\n",
    "water2020_join_name = water2020_join_auid.merge(hydro_geometry, how = \"left\", on = \"NAME_x\")\n",
    "\n",
    "hydro_geometry2 = hydro_clean.loc[hydro_clean[\"NAME_x\"] == \"Laura\"]\n",
    "\n",
    "hydro_geometry2 = hydro_geometry2[[\"NAME_x\", \"geometry\", \"status\"]]\n",
    "\n",
    "water2020_join_name2 = water2020_join_name.merge(hydro_geometry2, how = \"left\", on = \"NAME_x\")\n",
    "water2020_join_name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
